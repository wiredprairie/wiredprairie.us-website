<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: What&#8217;s the perfect API?	</title>
	<atom:link href="blog/index.php/archives/175/feed" rel="self" type="application/rss+xml" />
	<link>/blog/index.php/archives/175</link>
	<description>Yet another tech blog.</description>
	<lastBuildDate>Sun, 11 May 2008 20:40:04 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0</generator>
	<item>
		<title>
		By: WiredPrairie - Dead Weight APIs?		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-112</link>

		<dc:creator><![CDATA[WiredPrairie - Dead Weight APIs?]]></dc:creator>
		<pubDate>Sun, 11 May 2008 20:40:04 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-112</guid>

					<description><![CDATA[[...] What&#8217;s the Perfect API? [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] What&#8217;s the Perfect API? [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: DrPizza		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-108</link>

		<dc:creator><![CDATA[DrPizza]]></dc:creator>
		<pubDate>Sun, 11 May 2008 11:42:48 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-108</guid>

					<description><![CDATA[&quot;I hope MS is seriously investigating virtualization as a solution to backward compatibility.&quot;
The big problem with virtualization is getting a streamlined experience for the user.  Although we see attempts to do this with e.g. Parallels, VMware, VirtualBox, which can stick a virtual Windows window onto your actual desktop, the integration is still a long way off 100%.  It&#039;s also quite a heavyweight solution; you don&#039;t want to have to boot up a VM each time you run a legacy program, especially since that &quot;legacy&quot; program could have been built and compiled a few minutes ago!

Certainly, virtualization has a role; Win16 needs virtualization these days (no Win16 is available on 64-bit Windows), but we&#039;re really not in a position where we could say, for example, &quot;virtualize all Win32 apps&quot;.

&quot;I think I’ll probe some contacts at MS regarding 64-bit .NET. I may not be able to blog about them (due to NDAs), but you’ve got me curious.&quot;
Well, it depends on what you want to do with &quot;64-bit&quot;.  Sure, you can create a lot of objects (it is a 64-bit process, after all), it&#039;s just that I&#039;d like to create huge objects instead of lots of objects.

I do think they should have been forward-looking and made array sizes use &#039;long&#039; rather than &#039;int&#039;.  Sure, it&#039;d take a hit on 32-bit systems.  I don&#039;t care; even in 2002 it was obvious that the days of 32-bit were numbered.  Look to the future, not to the past.]]></description>
			<content:encoded><![CDATA[<p>&#8220;I hope MS is seriously investigating virtualization as a solution to backward compatibility.&#8221;<br />
The big problem with virtualization is getting a streamlined experience for the user.  Although we see attempts to do this with e.g. Parallels, VMware, VirtualBox, which can stick a virtual Windows window onto your actual desktop, the integration is still a long way off 100%.  It&#8217;s also quite a heavyweight solution; you don&#8217;t want to have to boot up a VM each time you run a legacy program, especially since that &#8220;legacy&#8221; program could have been built and compiled a few minutes ago!</p>
<p>Certainly, virtualization has a role; Win16 needs virtualization these days (no Win16 is available on 64-bit Windows), but we&#8217;re really not in a position where we could say, for example, &#8220;virtualize all Win32 apps&#8221;.</p>
<p>&#8220;I think I’ll probe some contacts at MS regarding 64-bit .NET. I may not be able to blog about them (due to NDAs), but you’ve got me curious.&#8221;<br />
Well, it depends on what you want to do with &#8220;64-bit&#8221;.  Sure, you can create a lot of objects (it is a 64-bit process, after all), it&#8217;s just that I&#8217;d like to create huge objects instead of lots of objects.</p>
<p>I do think they should have been forward-looking and made array sizes use &#8216;long&#8217; rather than &#8216;int&#8217;.  Sure, it&#8217;d take a hit on 32-bit systems.  I don&#8217;t care; even in 2002 it was obvious that the days of 32-bit were numbered.  Look to the future, not to the past.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: WiredPrairie - Perfect API&#8217;s &#8230; continuation		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-105</link>

		<dc:creator><![CDATA[WiredPrairie - Perfect API&#8217;s &#8230; continuation]]></dc:creator>
		<pubDate>Sun, 11 May 2008 03:24:24 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-105</guid>

					<description><![CDATA[[...] you were interested in the blog post, &#8220;What&#8217;s the Perfect API?&#8220;, (which related to blog post about messed up Win32 is &#8230;), you might want to head there [...]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] you were interested in the blog post, &#8220;What&#8217;s the Perfect API?&#8220;, (which related to blog post about messed up Win32 is &#8230;), you might want to head there [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Aaron		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-104</link>

		<dc:creator><![CDATA[Aaron]]></dc:creator>
		<pubDate>Sat, 10 May 2008 22:15:48 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-104</guid>

					<description><![CDATA[I hope MS is seriously investigating virtualization as a solution to backward compatibility.  

I think I&#039;ll probe some contacts at MS regarding 64-bit .NET. I may not be able to blog about them (due to NDAs), but you&#039;ve got me curious.]]></description>
			<content:encoded><![CDATA[<p>I hope MS is seriously investigating virtualization as a solution to backward compatibility.  </p>
<p>I think I&#8217;ll probe some contacts at MS regarding 64-bit .NET. I may not be able to blog about them (due to NDAs), but you&#8217;ve got me curious.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: DrPizza		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-102</link>

		<dc:creator><![CDATA[DrPizza]]></dc:creator>
		<pubDate>Sat, 10 May 2008 21:29:52 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-102</guid>

					<description><![CDATA[&quot;Versioning is a tough nut to crack. I don’t honestly know if that would help or hurt them — if they had 3 or 4 different versions of the WinAPI for example — would they be hampered by the sheer magnitude of keeping things aligned and supported?&quot;
I think some parts will be easier than others.  An example of something that should be fairly easy is the various control libraries there are.  Although these are dependent quite heavily on the mechanics of User32 (WndProcs, window messages, all that jazz), they should have zero kernel dependencies, and should be independent of any other parts of Windows.  It would seem quite feasible to freeze and version these.

Things would get a bit more tricky if we were talking something more ambitious--a wholesale replacement (rather than merely a tidying up) of User32 with something good, say--so that would require more effort (one might envisage a User32-workalike layer written to sit on top of WPF, say) but I think could still ultimately be achieved.

Please don&#039;t get me wrong: I&#039;m not suggesting that this would *eliminate* the issues MS faces.  Rather, I&#039;m suggesting that it would give them greater scope to move forward without jeopardizing their legacy, by compartmentalizing and isolating it.  And I find it incredibly frustrating that even though MS has good infrastructure for this with .NET, it&#039;s not being used.

&quot;A 4GB image file?! Wow! What kind of file is it? Video comes to mind, but you said image, and I’m curious as to what file type it is now&quot;
Oh, for work we get all kinds of huge TIFFs that we would like to manipulate in certain ways (typically convert them to other formats so that their file sizes are a bit smaller...).  Standard TIFF is in principle good for up to 4 GiB (though many implementations are restricted to signed 32-bit, i.e. 2 GiB), and there are proposed extensions for 64-bit TIFFs, BigTIFF, and there are certainly applications for such files.  Again, this is not to say that creating a bloody great array to hold all the image data is necessarily the best approach to use for such files (especially as they can easily outstrip any reasonably likely amount of memory), but even if you&#039;re dividing the images into chunks, you&#039;ll often want chunks as big as possible.]]></description>
			<content:encoded><![CDATA[<p>&#8220;Versioning is a tough nut to crack. I don’t honestly know if that would help or hurt them — if they had 3 or 4 different versions of the WinAPI for example — would they be hampered by the sheer magnitude of keeping things aligned and supported?&#8221;<br />
I think some parts will be easier than others.  An example of something that should be fairly easy is the various control libraries there are.  Although these are dependent quite heavily on the mechanics of User32 (WndProcs, window messages, all that jazz), they should have zero kernel dependencies, and should be independent of any other parts of Windows.  It would seem quite feasible to freeze and version these.</p>
<p>Things would get a bit more tricky if we were talking something more ambitious&#8211;a wholesale replacement (rather than merely a tidying up) of User32 with something good, say&#8211;so that would require more effort (one might envisage a User32-workalike layer written to sit on top of WPF, say) but I think could still ultimately be achieved.</p>
<p>Please don&#8217;t get me wrong: I&#8217;m not suggesting that this would *eliminate* the issues MS faces.  Rather, I&#8217;m suggesting that it would give them greater scope to move forward without jeopardizing their legacy, by compartmentalizing and isolating it.  And I find it incredibly frustrating that even though MS has good infrastructure for this with .NET, it&#8217;s not being used.</p>
<p>&#8220;A 4GB image file?! Wow! What kind of file is it? Video comes to mind, but you said image, and I’m curious as to what file type it is now&#8221;<br />
Oh, for work we get all kinds of huge TIFFs that we would like to manipulate in certain ways (typically convert them to other formats so that their file sizes are a bit smaller&#8230;).  Standard TIFF is in principle good for up to 4 GiB (though many implementations are restricted to signed 32-bit, i.e. 2 GiB), and there are proposed extensions for 64-bit TIFFs, BigTIFF, and there are certainly applications for such files.  Again, this is not to say that creating a bloody great array to hold all the image data is necessarily the best approach to use for such files (especially as they can easily outstrip any reasonably likely amount of memory), but even if you&#8217;re dividing the images into chunks, you&#8217;ll often want chunks as big as possible.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Aaron		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-100</link>

		<dc:creator><![CDATA[Aaron]]></dc:creator>
		<pubDate>Sat, 10 May 2008 20:34:42 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-100</guid>

					<description><![CDATA[Versioning is a tough nut to crack. I don&#039;t honestly know if that would help or hurt them -- if they had 3 or 4 different versions of the WinAPI for example -- would they be hampered by the sheer magnitude of keeping things aligned and supported? 

MMF&#039;s rock. They still haven&#039;t created a wrapper for them ... bummer. I wrote one a few years ago, but I&#039;d much rather it just be part of the core .NET framework. 

A 4GB image file?! Wow! What kind of file is it? Video comes to mind, but you said image, and I&#039;m curious as to what file type it is now .... :)]]></description>
			<content:encoded><![CDATA[<p>Versioning is a tough nut to crack. I don&#8217;t honestly know if that would help or hurt them &#8212; if they had 3 or 4 different versions of the WinAPI for example &#8212; would they be hampered by the sheer magnitude of keeping things aligned and supported? </p>
<p>MMF&#8217;s rock. They still haven&#8217;t created a wrapper for them &#8230; bummer. I wrote one a few years ago, but I&#8217;d much rather it just be part of the core .NET framework. </p>
<p>A 4GB image file?! Wow! What kind of file is it? Video comes to mind, but you said image, and I&#8217;m curious as to what file type it is now &#8230;. :)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: DrPizza		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-97</link>

		<dc:creator><![CDATA[DrPizza]]></dc:creator>
		<pubDate>Sat, 10 May 2008 16:26:02 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-97</guid>

					<description><![CDATA[&quot;DrPizza — thanks for the comments. But I fail to see how any of this actually addresses some of the hard issues — like running a business and trying to make money — which is what their shareholders want. &quot;
If I were an MS shareholder, I would be extremely dissatisfied at (a) Windows Vista (b) the failure of the share price to move upwards.  I don&#039;t think MS *is* delivering what its shareholders want.  And I don&#039;t think it *can* because it is so hamstrung by backwards compatibility.

I think Vista is the best Windows ever, and it&#039;s the OS I use most of the time (by far).  But it took more than *five years* from the release of XP to get there.  And there&#039;s no way that Vista is five years of improvement over XP.  It&#039;s no wonder that home and business users alike are saying &quot;You know what?  XP is good enough.  Vista&#039;s not offering anything worthwhile; it&#039;s just bigger, slower, and spuriously different&quot;.  There *are* good things about Vista (though many of them not obviously visible; I wrote at length about them at Ars), but there&#039;s very little that&#039;ll make someone who&#039;s of two minds think &quot;gosh, I really must have that&quot;.

MS&#039;s slavish adherence to backwards compatibility--and they *way* in which that BC is achieved--is preventing MS from giving value to its shareholders.

&quot;It’s not a lack of imagination that I have (if you knew me, you’d never suggest that) — it’s the knowledge and first hand experience of the hard truth of doing software development and trying to make money — sometimes you have to ship, even when it’s not what you wanted. You are absolutely underestimating the power of enterprise on Microsoft’s ship and design schedule. The enterprise plays a HUGE role in their every-day decisions. I know for sure — as I have worked there. &quot;
Sure.  But that&#039;s kind of missing the point, I think.

There are ways to provide backwards compatibility that do not hamstring future development.  The most obvious thing is *version your libraries*.  It&#039;s OK to freeze an old library and introduce a new one.  That doesn&#039;t break old applications, and equally, it doesn&#039;t cripple new ones.  But MS doesn&#039;t do that, which makes its development much, much harder.  Any time MS *does* try to introduce a new feature, MS has to make sure that it doesn&#039;t upset old programs.  Why?  Because the new feature has to be shoehorned in amongst the old stuff.

This approach simply doesn&#039;t scale well, as MS has discovered on countless previous occasions.

I&#039;m not claiming versioning is a panacea; if the change is big enough it can be very difficult to continue to support the old versions, no matter how good your intentions are (although we have good solutions for that, too--virtualization).  But it would provide a hell of a lot more flexibility than MS currently has.

&quot;How much time do think it would take to create a new 64 bit API with a compatibility layer for 32 bit? And how much testing would it require, by both Microsoft and 3rd parties? I can’t fathom it — it’s so high. &quot;
How much time?  Less than it took to deliver Vista, that&#039;s for sure.

&quot;What do you want from a 64 bit API anyway? What’s the real issue?&quot;
I want to write 64-bit applications.

&quot;If you use .NET, you’d normally not have to worry (unless doing PInvokes).&quot;
That&#039;s unfortunately not true.  .NET is, unfortunately, 32-bit.  Oh, sure, if you run a .NET program on 64-bit Windows it&#039;ll create a 64-bit process.  But you can&#039;t, say, allocate an array that&#039;s more than 2^31 elements long.  This is annoying, to say the least; I&#039;d like to, for example, work with large image files, and the ability to create 4 GiB byte arrays would make my coding *considerably* simpler.  You can take a look for yourself if you don&#039;t believe me; load up mscorlib.dll from the Framework64 directory and take a look at System.Array.LongLength.  It just casts the signed 32-bit Length property to long.  To deal with these large images, I&#039;d have to do some kind of a chunking mechanism.  That&#039;s a huge amount of extra complexity.

With native code, I don&#039;t have an issue.  I can just create a 4 GiB array.  I&#039;m not saying I would necessarily want to do things this way--although it probably *is* the most efficient way on large-memory systems--but it&#039;d be nice to have the option.  To compound the annoyance, .NET doesn&#039;t even have a good memory-mapped file mechanism (MMFs are another good way of handling large data structures).  If I want to create lots of little objects (i.e. more than 4 GiB worth of them), I suspect .NET would be good enough, but if I want to create a few huge objects, it actually kind of falls down.

And then there&#039;s all the things that I just can&#039;t access in .NET anyway.  Like DirectShow, or Media Foundation, or Core Audio.  These are off-limits whether 32- or 64-bit, because they just don&#039;t exist in .NET.  P/Invoke is quite good if I want to bring a couple of C API calls into the .NET world, but it&#039;s pretty grim if I want to bring whole APIs with complex data structures over.  C++/CLI is a much better option, because it lets me eschew the .NET things entirely.  I can ignore .NET Collections and arrays and ValueTypes and just use the C and C++ things directly.

&quot;Microsoft maintains a level of compatibility, even with .NET to make it easier to upgrade. If they didn’t, you’d find developers wouldn’t upgrade. The amount of effort required would exceed their schedules, the perceived benefit may be limited, etc. &quot;
I don&#039;t agree.  I think .NET provides pretty good infrastructure to support MS making breaking changes (especially if they have one version where things are deprecated and a second to remove them; this gives developers plenty of advance notice and opportunity to get their house in order).  With .NET&#039;s versioning, breaking changes will only break applications that are *actively maintained* (and hence rebuilt against the latest libraries).  They won&#039;t break legacy applications.  To me (and the enterprise development I have done), that seems like a more than acceptable compromise.

&quot;The harsh truth is that it’s Microsoft’s customers and developers who are preventing them from moving forward. Look at the number of people who are clinging to XP. How can they move the OS forward if people refuse to change? Imagine if they made a huge jump and all of their favorite software stopped working. It’s just not a simple problem to solve. There is no right answer.&quot;
The reason that people are clinging to XP is because Vista simply isn&#039;t that big a leap (in spite of its extended gestation).  A key reason that it&#039;s not a big leap is because its development was so restricted by backwards compatibility.]]></description>
			<content:encoded><![CDATA[<p>&#8220;DrPizza — thanks for the comments. But I fail to see how any of this actually addresses some of the hard issues — like running a business and trying to make money — which is what their shareholders want. &#8221;<br />
If I were an MS shareholder, I would be extremely dissatisfied at (a) Windows Vista (b) the failure of the share price to move upwards.  I don&#8217;t think MS *is* delivering what its shareholders want.  And I don&#8217;t think it *can* because it is so hamstrung by backwards compatibility.</p>
<p>I think Vista is the best Windows ever, and it&#8217;s the OS I use most of the time (by far).  But it took more than *five years* from the release of XP to get there.  And there&#8217;s no way that Vista is five years of improvement over XP.  It&#8217;s no wonder that home and business users alike are saying &#8220;You know what?  XP is good enough.  Vista&#8217;s not offering anything worthwhile; it&#8217;s just bigger, slower, and spuriously different&#8221;.  There *are* good things about Vista (though many of them not obviously visible; I wrote at length about them at Ars), but there&#8217;s very little that&#8217;ll make someone who&#8217;s of two minds think &#8220;gosh, I really must have that&#8221;.</p>
<p>MS&#8217;s slavish adherence to backwards compatibility&#8211;and they *way* in which that BC is achieved&#8211;is preventing MS from giving value to its shareholders.</p>
<p>&#8220;It’s not a lack of imagination that I have (if you knew me, you’d never suggest that) — it’s the knowledge and first hand experience of the hard truth of doing software development and trying to make money — sometimes you have to ship, even when it’s not what you wanted. You are absolutely underestimating the power of enterprise on Microsoft’s ship and design schedule. The enterprise plays a HUGE role in their every-day decisions. I know for sure — as I have worked there. &#8221;<br />
Sure.  But that&#8217;s kind of missing the point, I think.</p>
<p>There are ways to provide backwards compatibility that do not hamstring future development.  The most obvious thing is *version your libraries*.  It&#8217;s OK to freeze an old library and introduce a new one.  That doesn&#8217;t break old applications, and equally, it doesn&#8217;t cripple new ones.  But MS doesn&#8217;t do that, which makes its development much, much harder.  Any time MS *does* try to introduce a new feature, MS has to make sure that it doesn&#8217;t upset old programs.  Why?  Because the new feature has to be shoehorned in amongst the old stuff.</p>
<p>This approach simply doesn&#8217;t scale well, as MS has discovered on countless previous occasions.</p>
<p>I&#8217;m not claiming versioning is a panacea; if the change is big enough it can be very difficult to continue to support the old versions, no matter how good your intentions are (although we have good solutions for that, too&#8211;virtualization).  But it would provide a hell of a lot more flexibility than MS currently has.</p>
<p>&#8220;How much time do think it would take to create a new 64 bit API with a compatibility layer for 32 bit? And how much testing would it require, by both Microsoft and 3rd parties? I can’t fathom it — it’s so high. &#8221;<br />
How much time?  Less than it took to deliver Vista, that&#8217;s for sure.</p>
<p>&#8220;What do you want from a 64 bit API anyway? What’s the real issue?&#8221;<br />
I want to write 64-bit applications.</p>
<p>&#8220;If you use .NET, you’d normally not have to worry (unless doing PInvokes).&#8221;<br />
That&#8217;s unfortunately not true.  .NET is, unfortunately, 32-bit.  Oh, sure, if you run a .NET program on 64-bit Windows it&#8217;ll create a 64-bit process.  But you can&#8217;t, say, allocate an array that&#8217;s more than 2^31 elements long.  This is annoying, to say the least; I&#8217;d like to, for example, work with large image files, and the ability to create 4 GiB byte arrays would make my coding *considerably* simpler.  You can take a look for yourself if you don&#8217;t believe me; load up mscorlib.dll from the Framework64 directory and take a look at System.Array.LongLength.  It just casts the signed 32-bit Length property to long.  To deal with these large images, I&#8217;d have to do some kind of a chunking mechanism.  That&#8217;s a huge amount of extra complexity.</p>
<p>With native code, I don&#8217;t have an issue.  I can just create a 4 GiB array.  I&#8217;m not saying I would necessarily want to do things this way&#8211;although it probably *is* the most efficient way on large-memory systems&#8211;but it&#8217;d be nice to have the option.  To compound the annoyance, .NET doesn&#8217;t even have a good memory-mapped file mechanism (MMFs are another good way of handling large data structures).  If I want to create lots of little objects (i.e. more than 4 GiB worth of them), I suspect .NET would be good enough, but if I want to create a few huge objects, it actually kind of falls down.</p>
<p>And then there&#8217;s all the things that I just can&#8217;t access in .NET anyway.  Like DirectShow, or Media Foundation, or Core Audio.  These are off-limits whether 32- or 64-bit, because they just don&#8217;t exist in .NET.  P/Invoke is quite good if I want to bring a couple of C API calls into the .NET world, but it&#8217;s pretty grim if I want to bring whole APIs with complex data structures over.  C++/CLI is a much better option, because it lets me eschew the .NET things entirely.  I can ignore .NET Collections and arrays and ValueTypes and just use the C and C++ things directly.</p>
<p>&#8220;Microsoft maintains a level of compatibility, even with .NET to make it easier to upgrade. If they didn’t, you’d find developers wouldn’t upgrade. The amount of effort required would exceed their schedules, the perceived benefit may be limited, etc. &#8221;<br />
I don&#8217;t agree.  I think .NET provides pretty good infrastructure to support MS making breaking changes (especially if they have one version where things are deprecated and a second to remove them; this gives developers plenty of advance notice and opportunity to get their house in order).  With .NET&#8217;s versioning, breaking changes will only break applications that are *actively maintained* (and hence rebuilt against the latest libraries).  They won&#8217;t break legacy applications.  To me (and the enterprise development I have done), that seems like a more than acceptable compromise.</p>
<p>&#8220;The harsh truth is that it’s Microsoft’s customers and developers who are preventing them from moving forward. Look at the number of people who are clinging to XP. How can they move the OS forward if people refuse to change? Imagine if they made a huge jump and all of their favorite software stopped working. It’s just not a simple problem to solve. There is no right answer.&#8221;<br />
The reason that people are clinging to XP is because Vista simply isn&#8217;t that big a leap (in spite of its extended gestation).  A key reason that it&#8217;s not a big leap is because its development was so restricted by backwards compatibility.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Aaron		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-95</link>

		<dc:creator><![CDATA[Aaron]]></dc:creator>
		<pubDate>Sat, 10 May 2008 14:43:59 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-95</guid>

					<description><![CDATA[DrPizza -- thanks for the comments. But I fail to see how any of this actually addresses some of the hard issues -- like running a business and trying to make money -- which is what their shareholders want. 

It&#039;s not a lack of imagination that I have (if you knew me, you&#039;d never suggest that) -- it&#039;s the knowledge and first hand experience of the hard truth of doing software development and trying to make money -- sometimes you have to ship, even when it&#039;s not what you wanted. You are absolutely underestimating the power of enterprise on Microsoft&#039;s ship and design schedule. The enterprise plays a HUGE role in their every-day decisions. I know for sure -- as I have worked there. 

How much time do think it would take to create a new 64 bit API with a compatibility layer for 32 bit? And how much testing would it require, by both Microsoft and 3rd parties? I can&#039;t fathom it -- it&#039;s so high. 

What do you want from a 64 bit API anyway? What&#039;s the real issue? If you use .NET, you&#039;d normally not have to worry (unless doing PInvokes). Microsoft maintains a level of compatibility, even with .NET to make it easier to upgrade. If they didn&#039;t, you&#039;d find developers wouldn&#039;t upgrade. The amount of effort required would exceed their schedules, the perceived benefit may be limited, etc. 

The harsh truth is that it&#039;s Microsoft&#039;s customers and developers who are preventing them from moving forward. Look at the number of people who are clinging to XP. How can they move the OS forward if people refuse to change? Imagine if they made a huge jump and all of their favorite software stopped working. It&#039;s just not a simple problem to solve. There is no right answer.

Again, thanks for your comments.]]></description>
			<content:encoded><![CDATA[<p>DrPizza &#8212; thanks for the comments. But I fail to see how any of this actually addresses some of the hard issues &#8212; like running a business and trying to make money &#8212; which is what their shareholders want. </p>
<p>It&#8217;s not a lack of imagination that I have (if you knew me, you&#8217;d never suggest that) &#8212; it&#8217;s the knowledge and first hand experience of the hard truth of doing software development and trying to make money &#8212; sometimes you have to ship, even when it&#8217;s not what you wanted. You are absolutely underestimating the power of enterprise on Microsoft&#8217;s ship and design schedule. The enterprise plays a HUGE role in their every-day decisions. I know for sure &#8212; as I have worked there. </p>
<p>How much time do think it would take to create a new 64 bit API with a compatibility layer for 32 bit? And how much testing would it require, by both Microsoft and 3rd parties? I can&#8217;t fathom it &#8212; it&#8217;s so high. </p>
<p>What do you want from a 64 bit API anyway? What&#8217;s the real issue? If you use .NET, you&#8217;d normally not have to worry (unless doing PInvokes). Microsoft maintains a level of compatibility, even with .NET to make it easier to upgrade. If they didn&#8217;t, you&#8217;d find developers wouldn&#8217;t upgrade. The amount of effort required would exceed their schedules, the perceived benefit may be limited, etc. </p>
<p>The harsh truth is that it&#8217;s Microsoft&#8217;s customers and developers who are preventing them from moving forward. Look at the number of people who are clinging to XP. How can they move the OS forward if people refuse to change? Imagine if they made a huge jump and all of their favorite software stopped working. It&#8217;s just not a simple problem to solve. There is no right answer.</p>
<p>Again, thanks for your comments.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: DrPizza		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-94</link>

		<dc:creator><![CDATA[DrPizza]]></dc:creator>
		<pubDate>Sat, 10 May 2008 11:31:15 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-94</guid>

					<description><![CDATA[Point 1: It&#039;s called hyperbole.  Yes, I&#039;m sure that there are individuals within MS who do care.  Maybe even a lot of them.  But I&#039;ll tell you this: creating a good, consistent, clean UI is clearly not a priority at MS.  Reinvention and duplication is not an occasional anomaly.  It is the norm.  The Windows Vista UI Guidelines, for example, are probably some of the most thorough MS has ever come up with.  And yet... and yet Windows Vista itself does not conform with its own guidelines.  Not just in a few places, either.  The non-conformance is *everywhere*.  The Vista UI Guidelines in a number of places use real Vista screenshots as *examples of what not to do*.

It may well be that people near the bottom of the totem pole care about this kind of thing, but it&#039;s also clear that people at the top--people who make decisions--do not.  There is no-one within the company saying &quot;This application&#039;s UI is needlessly non-standard; it reinvents this, this, and this, it&#039;s a pain for users and it&#039;s a pain for maintenance--go fix it&quot;.  It just isn&#039;t seen as a priority.

Point 2: &quot;Dude, do you seriously want to conditionalize all of your code just so you can recompile on 64 bit Windows? Do you want a new API just for that? Seriously, what’s the gain?&quot;.  No, I don&#039;t want that.  And I think it shows an incredible lack of imagination on your part that you think that&#039;s the only alternative.

What I would like to see is a clean 64-bit API, and a 32-bit compatibility library, so I could write 32-bit apps against a workalike of the 64-bit API.  This isn&#039;t unprecedented, at all; Microsoft created the Unicode layer for Windows 9x, so that programs could be written to use (NT&#039;s preferred) Unicode APIs against the (narrow char/multibyte) Windows 9x family.  Apple produced CarbonLib to allow programs to use Mac OS X&#039;s (semi-modern) Carbon APIs on (legacy) MacOS 9.  This isn&#039;t a totally outlandish, never-seen-before concept.  It&#039;s something that both Apple and MS have done to ease transitions.

Point 3: It is precisely because Apple to such an extent started with a clean slate that Apple has been able to be much better on keeping APIs clean and powerful.  Apple is also willing to deprecate and remove things, which MS never does.  This is particularly upsetting in .NET.  .NET, unlike Windows proper, has a pretty good versioning mechanism built-in.  SxS installation, rolling forward dependencies on minor version changes, that&#039;s all there already.  There&#039;s no reason to keep everything old and deprecated hanging around for ever with .NET, and yet that&#039;s the road they&#039;re going down anyway.

Point 4: No, they don&#039;t need to all ship on the same schedule.  But you are mistaken if you think that their development is &quot;innovat[ion]&quot; or producing something &quot;unique&quot;.  The no-menu thing in WMP, Explorer, IE, WLM, Photo Gallery, etc.--that&#039;s not unique.  It&#039;s just coded a bunch of different times.  That&#039;s not providing any advantage to any user.  It&#039;s not even providing any advantage to MS (because it&#039;s just that much more code to maintain).  The reason MS is doing that is simply because there&#039;s no co-ordination between divisions.  The left hand is totally clueless about the right hand.

Now, maybe we could let that slide for stuff like WLM, which is a separate download anyway (though I still think first party apps should do a lot better than MS&#039;s do).  But for the things that *all ship together on the Vista DVD* to suffer this same reinvention and inconsistency?  That stuff *already* has the same ship schedule.  It&#039;s *already* being combined into a single release.  It is *not* asking a lot for it to be even slightly consistent.]]></description>
			<content:encoded><![CDATA[<p>Point 1: It&#8217;s called hyperbole.  Yes, I&#8217;m sure that there are individuals within MS who do care.  Maybe even a lot of them.  But I&#8217;ll tell you this: creating a good, consistent, clean UI is clearly not a priority at MS.  Reinvention and duplication is not an occasional anomaly.  It is the norm.  The Windows Vista UI Guidelines, for example, are probably some of the most thorough MS has ever come up with.  And yet&#8230; and yet Windows Vista itself does not conform with its own guidelines.  Not just in a few places, either.  The non-conformance is *everywhere*.  The Vista UI Guidelines in a number of places use real Vista screenshots as *examples of what not to do*.</p>
<p>It may well be that people near the bottom of the totem pole care about this kind of thing, but it&#8217;s also clear that people at the top&#8211;people who make decisions&#8211;do not.  There is no-one within the company saying &#8220;This application&#8217;s UI is needlessly non-standard; it reinvents this, this, and this, it&#8217;s a pain for users and it&#8217;s a pain for maintenance&#8211;go fix it&#8221;.  It just isn&#8217;t seen as a priority.</p>
<p>Point 2: &#8220;Dude, do you seriously want to conditionalize all of your code just so you can recompile on 64 bit Windows? Do you want a new API just for that? Seriously, what’s the gain?&#8221;.  No, I don&#8217;t want that.  And I think it shows an incredible lack of imagination on your part that you think that&#8217;s the only alternative.</p>
<p>What I would like to see is a clean 64-bit API, and a 32-bit compatibility library, so I could write 32-bit apps against a workalike of the 64-bit API.  This isn&#8217;t unprecedented, at all; Microsoft created the Unicode layer for Windows 9x, so that programs could be written to use (NT&#8217;s preferred) Unicode APIs against the (narrow char/multibyte) Windows 9x family.  Apple produced CarbonLib to allow programs to use Mac OS X&#8217;s (semi-modern) Carbon APIs on (legacy) MacOS 9.  This isn&#8217;t a totally outlandish, never-seen-before concept.  It&#8217;s something that both Apple and MS have done to ease transitions.</p>
<p>Point 3: It is precisely because Apple to such an extent started with a clean slate that Apple has been able to be much better on keeping APIs clean and powerful.  Apple is also willing to deprecate and remove things, which MS never does.  This is particularly upsetting in .NET.  .NET, unlike Windows proper, has a pretty good versioning mechanism built-in.  SxS installation, rolling forward dependencies on minor version changes, that&#8217;s all there already.  There&#8217;s no reason to keep everything old and deprecated hanging around for ever with .NET, and yet that&#8217;s the road they&#8217;re going down anyway.</p>
<p>Point 4: No, they don&#8217;t need to all ship on the same schedule.  But you are mistaken if you think that their development is &#8220;innovat[ion]&#8221; or producing something &#8220;unique&#8221;.  The no-menu thing in WMP, Explorer, IE, WLM, Photo Gallery, etc.&#8211;that&#8217;s not unique.  It&#8217;s just coded a bunch of different times.  That&#8217;s not providing any advantage to any user.  It&#8217;s not even providing any advantage to MS (because it&#8217;s just that much more code to maintain).  The reason MS is doing that is simply because there&#8217;s no co-ordination between divisions.  The left hand is totally clueless about the right hand.</p>
<p>Now, maybe we could let that slide for stuff like WLM, which is a separate download anyway (though I still think first party apps should do a lot better than MS&#8217;s do).  But for the things that *all ship together on the Vista DVD* to suffer this same reinvention and inconsistency?  That stuff *already* has the same ship schedule.  It&#8217;s *already* being combined into a single release.  It is *not* asking a lot for it to be even slightly consistent.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: JulesLt		</title>
		<link>/blog/index.php/archives/175/comment-page-1#comment-77</link>

		<dc:creator><![CDATA[JulesLt]]></dc:creator>
		<pubDate>Wed, 07 May 2008 22:32:16 +0000</pubDate>
		<guid isPermaLink="false">/blog/index.php/archives/175#comment-77</guid>

					<description><![CDATA[Well, Nextstep dates from the early 90s - it was one of the first strict MVC frameworks - and the decision to use Obj-C for that must have been made slightly earlier. 

At that point in time, C++ was still also a young language, and Obj-C was actually more compatible with ANSI C than C++ (i.e. it was a strict extension to C).

The conceptual implementation of OO in Obj-C was also better - it is closer to Smalltalk - whereas C++ (and by extension Java) compromised OO in favour of performance. At the time, that was a fair trade-off - NextStep only ran on workstation class machines around the $10,000 price point, whereas C++ apps could run on anything. There was no &#039;C++ runtime&#039; that they ran in. 

On the downside, that also means shared libraries rather than shared components and objects. 

I think that is the particular strength of Cocoa - the late (runtime rather than compile time) binding makes it easy for existing applications to share components, or pick up new O/S features without recompilation - for example, the standard text class was extended under Tiger to include Spotlight/Google search options, and under Leopard a grammar checker. 

My understanding from the debate around the Java bridge, and the more recent Python and Ruby bridges, is that it&#039;s easier to build a bridge to a more dynamic language, than to a less dynamic one - in much the same way that it is easier to call C from C++ than the other way round. 

I do wonder about your first question - given the origins of Cocoa and Obj-C date back 15 years, I do wonder what a new clean API based around those 15 years of experience would look like.

Anyway, that&#039;s getting away from the API debate!]]></description>
			<content:encoded><![CDATA[<p>Well, Nextstep dates from the early 90s &#8211; it was one of the first strict MVC frameworks &#8211; and the decision to use Obj-C for that must have been made slightly earlier. </p>
<p>At that point in time, C++ was still also a young language, and Obj-C was actually more compatible with ANSI C than C++ (i.e. it was a strict extension to C).</p>
<p>The conceptual implementation of OO in Obj-C was also better &#8211; it is closer to Smalltalk &#8211; whereas C++ (and by extension Java) compromised OO in favour of performance. At the time, that was a fair trade-off &#8211; NextStep only ran on workstation class machines around the $10,000 price point, whereas C++ apps could run on anything. There was no &#8216;C++ runtime&#8217; that they ran in. </p>
<p>On the downside, that also means shared libraries rather than shared components and objects. </p>
<p>I think that is the particular strength of Cocoa &#8211; the late (runtime rather than compile time) binding makes it easy for existing applications to share components, or pick up new O/S features without recompilation &#8211; for example, the standard text class was extended under Tiger to include Spotlight/Google search options, and under Leopard a grammar checker. </p>
<p>My understanding from the debate around the Java bridge, and the more recent Python and Ruby bridges, is that it&#8217;s easier to build a bridge to a more dynamic language, than to a less dynamic one &#8211; in much the same way that it is easier to call C from C++ than the other way round. </p>
<p>I do wonder about your first question &#8211; given the origins of Cocoa and Obj-C date back 15 years, I do wonder what a new clean API based around those 15 years of experience would look like.</p>
<p>Anyway, that&#8217;s getting away from the API debate!</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
